import os
import sys

PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../'))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
    
import google.generativeai as genai
from rest_framework.response import Response
from rest_framework.decorators import api_view
from django.shortcuts import get_object_or_404
from .models import ChatMessage, Conversation
from .serializers import ChatMessageSerializer, ConversationSerializer
from rag.hybrid_search import HybridSearchQdrant, extract_field_department_year, count_keywords_by_category
from sentence_transformers import SentenceTransformer
from rag.keywords import keywords_dict
from datetime import datetime
import pytz
import torch
import numpy as np
from typing import List, Dict
from qdrant_client import QdrantClient
from dotenv import load_dotenv

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
os.chdir(CURRENT_DIR)

dotenv_path = os.path.join(PROJECT_ROOT, '.env')
load_dotenv(dotenv_path)

# --- Qdrant c·∫•u h√¨nh ---
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
COLLECTION_NAME = os.getenv("COLLECTION_NAME")
EMBEDDING_MODEL = "AITeamVN/Vietnamese_Embedding"
MAX_LENGTH = 2048

# Qdrant client v√† model
qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)
embedding_model = SentenceTransformer(EMBEDDING_MODEL)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
embedding_model = embedding_model.to(device)
embedding_model.eval()

hybrid_search_engine = HybridSearchQdrant(
    qdrant_url=QDRANT_URL,
    qdrant_api_key=QDRANT_API_KEY,
    collection_name=COLLECTION_NAME,
    embedding_model=embedding_model,
    metadata_weight=0.2,
    semantic_weight=0.8
)

def retrieve_documents(query: str, top_k: int = 5) -> List[Dict]:
    try:
        field, filter_keywords, found_keywords, department, year = extract_field_department_year(query, keywords_dict)
        results = hybrid_search_engine.search(
            query=query,
            filter_keywords=filter_keywords,
            field=field,
            department=department,
            year=year,
            top_k=top_k
        )
        
        for temp_doc in results:
            if temp_doc.get("combined_score", 0) < 0.45:
                print(f"‚ö†Ô∏è B·ªè qua t√†i li·ªáu {temp_doc.get('title', 'kh√¥ng c√≥ ti√™u ƒë·ªÅ')} do ƒëi·ªÉm qu√° th·∫•p: {temp_doc.get('combined_score', 0)}")

        return [
            {
                "score": doc.get("combined_score", 0),
                "title": doc.get("title", ""),
                "content": doc.get("content", ""),
                "source": doc.get("source_file", "")
            }
            for doc in results if doc.get("combined_score", 0) >= 0.45
        ]
    except Exception as e:
        print(f"L·ªói khi truy v·∫•n Qdrant (hybrid): {e}")
        return []

def get_markdown_content_from_sources(source_files: set) -> str:
    """ƒê·ªçc to√†n b·ªô n·ªôi dung c√°c file markdown ngu·ªìn."""
    content = ""
    for src in source_files:
        md_path = os.path.join("markdown_data", src)
        if os.path.exists(md_path):
            with open(md_path, "r", encoding="utf-8") as f:
                content += f"\n---\nFile: {src}\n" + f.read()
    return content

def format_response(documents: List[Dict]) -> str:
    if not documents:
        return "Kh√¥ng t√¨m th·∫•y t√†i li·ªáu ph√π h·ª£p."
    response = ""
    for i, doc in enumerate(documents):
        response += f"\nT√†i li·ªáu {i} (score {doc['score']:.2f}):\nTi√™u ƒë·ªÅ: {doc['title']}\nN·ªôi dung: {doc['content']}\n"
    return response

import time
# ...existing code...

def get_chat_response(user_message, history):
    model = genai.GenerativeModel("gemini-2.0-flash")
    tz = pytz.timezone("Asia/Ho_Chi_Minh")
    current_date = datetime.now(tz).strftime("%d/%m/%Y %H:%M:%S")
    generation_config = {
        "temperature": 0.3,
        "max_output_tokens": 2048,
        "top_k": 20,
        "top_p": 0.95,
    }

    t0 = time.time()
    documents = retrieve_documents(user_message)
    t1 = time.time()
    docs_summary = format_response(documents)
    print(documents)
    # T·∫°o set c√°c file ngu·ªìn tr∆∞·ªõc
    source_files = set(doc["source"] for doc in documents if doc.get("source"))
    # Sau ƒë√≥ m·ªõi load n·ªôi dung c√°c file markdown
    full_markdown_content = get_markdown_content_from_sources(source_files)
    t2 = time.time()

    base_prompt = f'''
    B·∫°n l√† m·ªôt chatbot, tr·ª£ l√Ω ·∫£o th√¥ng minh v√† ƒë∆∞·ª£c sinh ra v·ªõi m·ª•c ƒë√≠ch t∆∞ v·∫•n tuy·ªÉn sinh cho tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng ngh·ªá Th√¥ng tin - ƒêHQG TP.HCM (UIT). B·∫°n c√≥ th·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn tuy·ªÉn sinh, ng√†nh h·ªçc, ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o v√† c√°c th√¥ng tin kh√°c li√™n quan ƒë·∫øn tr∆∞·ªùng d·ª±a v√†o c√°c t√†i li·ªáu tham kh·∫£o.
    *QUAN TR·ªåNG*: B·∫°n ph·∫£i s·ª≠ d·ª•ng to√†n b·ªô n·ªôi dung t·ª´ c√°c t√†i li·ªáu tham kh·∫£o ƒë·ªÉ tr·∫£ l·ªùi. H√£y ƒë√°nh gi√°, so s√°nh v√† t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu t√†i li·ªáu n·∫øu c·∫ßn thi·∫øt ƒë·ªÉ t·∫°o ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß nh·∫•t.

    *Nguy√™n t·∫Øc b·∫Øt bu·ªôc*:
    1. T·ª´ t·∫•t c·∫£ c√°c t√†i li·ªáu tham kh·∫£o, b·∫°n c·∫ßn ƒë·ªçc h·∫øt m·ªôt c√°ch chi ti·∫øt c√°c t√†i li·ªáu ƒë√≥ sau ƒë√≥ x√°c ƒë·ªãnh c√¢u h·ªèi c√≥ li√™n quan ƒë·∫øn t·∫•t c·∫£ t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p kh√¥ng v√† tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c, ƒë·∫ßy ƒë·ªß nh·∫•t.
    2. N·∫øu c√¢u h·ªèi li√™n quan ƒë·∫øn c√°c t√†i li·ªáu hi·ªán t·∫°i, b·∫°n c·∫ßn tr·∫£ l·ªùi d·ª±a tr√™n c√°c t√†i li·ªáu ƒë√£ ƒë∆∞·ª£c cung c·∫•p.
    3. N·∫øu c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn t√†i li·ªáu hi·ªán t·∫°i, b·∫°n v·∫´n c√≥ th·ªÉ tr·∫£ l·ªùi b·∫±ng ki·∫øn th·ª©c chung, nh∆∞ng ph·∫£i m·ªü ƒë·∫ßu r√µ r√†ng:
    ‚Üí "C√¢u h·ªèi n√†y kh√¥ng n·∫±m trong c√°c t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p. T√¥i s·∫Ω s·ª≠ d·ª•ng ki·∫øn th·ª©c chung ƒë·ªÉ tr·∫£ l·ªùi:"
    4. S·ª≠ d·ª•ng font Unicode ti√™u chu·∫©n.
    5. N·∫øu ng∆∞·ªùi d√πng h·ªèi b·∫±ng ng√¥n ng·ªØ kh√°c, kh√¥ng ph·∫£i ti·∫øng Vi·ªát, h√£y h·ªèi l·∫°i l·ªãch s·ª±:
    ‚Üí "B·∫°n c√≥ mu·ªën t√¥i tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát kh√¥ng?"

    üìÖ Ng√†y hi·ªán t·∫°i: {current_date}
    üè´ Tr∆∞·ªùng: ƒê·∫°i h·ªçc C√¥ng ngh·ªá Th√¥ng tin - ƒêHQG TP.HCM (UIT)
    üìö Danh s√°ch c√°c t√†i li·ªáu tham kh·∫£o (c√°c ƒëo·∫°n li√™n quan nh·∫•t):
    {docs_summary}

    üìÇ N·ªôi dung ƒë·∫ßy ƒë·ªß c·ªßa c√°c file t√†i li·ªáu ngu·ªìn (h√£y ƒë·ªçc k·ªπ ƒë·ªÉ tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß nh·∫•t):
    {full_markdown_content}
    '''

    history_text = "".join(
        f"Ng∆∞·ªùi d√πng: {msg.user_message}\nChatbot_uni: {msg.bot_response}\n" for msg in history
    )
    new_message = f"Ng∆∞·ªùi d√πng: {user_message}\nChatbot_uni:"

    full_content = base_prompt + "\n"

    if len(history_text) + len(new_message) > 12000:
        full_content += "L·ªãch s·ª≠ h·ªôi tho·∫°i qu√° d√†i.\n"

    full_content += history_text + new_message

    # In th·ªùi gian truy v·∫•n v√† augmentation
    print(f"‚è±Ô∏è Th·ªùi gian truy v·∫•n (Qdrant): {t1 - t0:.3f} gi√¢y")
    print(f"‚è±Ô∏è Th·ªùi gian augmentation (load markdown & chu·∫©n b·ªã prompt): {t2 - t1:.3f} gi√¢y")

    response = model.generate_content(full_content, generation_config=generation_config)
    final_text = response.text
    
    return final_text

# --- API Views ---
@api_view(['GET', 'POST', 'DELETE'])
def conversation_handler(request):
    user_id = request.headers.get('X-User-ID')
    if not user_id:
        return Response({'error': 'Unauthorized'}, status=401)

    if request.method == "GET":
        conversation_index = request.GET.get("conversation_index")
        conversation = get_object_or_404(Conversation, conversation_index=conversation_index, user_id=user_id)
        messages = ChatMessage.objects.filter(conversation=conversation).order_by("index")
        return Response({
            "conversation_index": conversation.conversation_index,
            "messages": ChatMessageSerializer(messages, many=True).data
        })

    elif request.method == "POST":
        message = request.data.get('message', '')
        conversation_index = request.GET.get("conversation_index") or request.data.get('conversation_index')

        if not message:
            return Response({'error': 'Message is required'}, status=400)

        if conversation_index:
            conversation = get_object_or_404(Conversation, conversation_index=conversation_index, user_id=user_id)
        else:
            conversation = Conversation.objects.create(user_id=user_id)

        current_message_count = ChatMessage.objects.filter(conversation=conversation).count()
        history_messages = ChatMessage.objects.filter(conversation=conversation).order_by("index")

        response = get_chat_response(message, history_messages)
        chat = ChatMessage.objects.create(
            conversation=conversation, index=current_message_count,
            user_message=message, bot_response=response
        )

        return Response({
            "conversation_index": conversation.conversation_index,
            "chat": ChatMessageSerializer(chat).data
        })

    elif request.method == "DELETE":
        conversation_index = request.GET.get("conversation_index") or request.data.get("conversation_index")
        conversation = get_object_or_404(Conversation, conversation_index=conversation_index, user_id=user_id)
        conversation.delete()
        return Response({"message": "Deleted successfully"}, status=200)

@api_view(['GET'])
def conversation_history(request):
    user_id = request.headers.get('X-User-ID')
    conversations = Conversation.objects.filter(user_id=user_id).order_by('-conversation_index')
    if not conversations.exists():
        return Response({"error": "Kh√¥ng c√≥ h·ªôi tho·∫°i n√†o."}, status=404)
    return Response(ConversationSerializer(conversations, many=True).data)

@api_view(['POST'])
def create_new_conversation(request):
    user_id = request.headers.get('X-User-ID')
    conversation = Conversation.objects.create(user_id=user_id)
    return Response({"conversation_index": conversation.conversation_index})
