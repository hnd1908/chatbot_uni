import os
import sys
import uuid

PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../'))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
    
import google.generativeai as genai
from rest_framework.response import Response
from rest_framework.decorators import api_view
from .models import ChatMessage, Conversation
from .serializers import ChatMessageSerializer, ConversationSerializer
from rag.hybrid_search import HybridSearchQdrant, extract_field_department_year, count_keywords_by_category
from sentence_transformers import SentenceTransformer
from rag.keywords import keywords_dict
from datetime import datetime
import pytz
import torch
import numpy as np
from typing import List, Dict
from qdrant_client import QdrantClient
from dotenv import load_dotenv
import time
from django.shortcuts import get_object_or_404

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
os.chdir(CURRENT_DIR)

dotenv_path = os.path.join(PROJECT_ROOT, '.env')
load_dotenv(dotenv_path)

QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
COLLECTION_NAME = os.getenv("COLLECTION_NAME")
EMBEDDING_MODEL = "AITeamVN/Vietnamese_Embedding"
MAX_LENGTH = 2048

# Qdrant client vÃ  model
qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)
embedding_model = SentenceTransformer(EMBEDDING_MODEL)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
embedding_model = embedding_model.to(device)
embedding_model.eval()

hybrid_search_engine = HybridSearchQdrant(
    qdrant_url=QDRANT_URL,
    qdrant_api_key=QDRANT_API_KEY,
    collection_name=COLLECTION_NAME,
    embedding_model=embedding_model,
    metadata_weight=0.2,
    semantic_weight=0.8
)

def retrieve_documents(query: str, top_k: int = 5) -> List[Dict]:
    try:
        field, filter_keywords, found_keywords, department, year = extract_field_department_year(query, keywords_dict)
        results = hybrid_search_engine.search(
            query=query,
            filter_keywords=filter_keywords,
            field=field,
            department=department,
            year=year,
            top_k=top_k
        )
        
        for temp_doc in results:
            if temp_doc.get("combined_score", 0) < 0.45:
                print(f"âš ï¸ Bá» qua tÃ i liá»‡u {temp_doc.get('title', 'khÃ´ng cÃ³ tiÃªu Ä‘á»')} do Ä‘iá»ƒm quÃ¡ tháº¥p: {temp_doc.get('combined_score', 0)}")

        return [
            {
                "score": doc.get("combined_score", 0),
                "title": doc.get("title", ""),
                "content": doc.get("content", ""),
                "source": doc.get("source_file", "")
            }
            for doc in results if doc.get("combined_score", 0) >= 0.45
        ]
    except Exception as e:
        print(f"Lá»—i khi truy váº¥n Qdrant (hybrid): {e}")
        return []

def get_markdown_content_from_sources(source_files: set) -> str:
    """Äá»c toÃ n bá»™ ná»™i dung cÃ¡c file markdown nguá»“n."""
    content = ""
    for src in source_files:
        md_path = os.path.join("markdown_data", src)
        if os.path.exists(md_path):
            with open(md_path, "r", encoding="utf-8") as f:
                content += f"\n---\nFile: {src}\n" + f.read()
    return content

def format_response(documents: List[Dict]) -> str:
    if not documents:
        return "KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u phÃ¹ há»£p."
    response = ""
    for i, doc in enumerate(documents):
        response += f"\nTÃ i liá»‡u {i} (score {doc['score']:.2f}):\nTiÃªu Ä‘á»: {doc['title']}\nNá»™i dung: {doc['content']}\n"
    return response


def get_chat_response(user_message, history=None):
    model = genai.GenerativeModel("gemini-2.0-flash")
    tz = pytz.timezone("Asia/Ho_Chi_Minh")
    current_date = datetime.now(tz).strftime("%d/%m/%Y %H:%M:%S")
    generation_config = {
        "temperature": 0.5,
        "max_output_tokens": 2048,
        "top_k": 20,
        "top_p": 0.95,
    }

    t0 = time.time()
    documents = retrieve_documents(user_message)
    t1 = time.time()
    docs_summary = format_response(documents)
    print(documents)
    source_files = set(doc["source"] for doc in documents if doc.get("source"))
    full_markdown_content = get_markdown_content_from_sources(source_files)
    t2 = time.time()

    base_prompt = f'''
    Báº¡n lÃ  má»™t chatbot, trá»£ lÃ½ áº£o thÃ´ng minh vÃ  Ä‘Æ°á»£c sinh ra vá»›i má»¥c Ä‘Ã­ch tÆ° váº¥n tuyá»ƒn sinh cho trÆ°á»ng Äáº¡i há»c CÃ´ng nghá»‡ ThÃ´ng tin - ÄHQG TP.HCM (UIT). Báº¡n cÃ³ thá»ƒ tráº£ lá»i cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n tuyá»ƒn sinh, ngÃ nh há»c, chÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o vÃ  cÃ¡c thÃ´ng tin khÃ¡c liÃªn quan Ä‘áº¿n trÆ°á»ng dá»±a vÃ o cÃ¡c tÃ i liá»‡u tham kháº£o.
    *QUAN TRá»ŒNG*: Báº¡n pháº£i sá»­ dá»¥ng toÃ n bá»™ ná»™i dung tá»« cÃ¡c tÃ i liá»‡u tham kháº£o Ä‘á»ƒ tráº£ lá»i. HÃ£y Ä‘Ã¡nh giÃ¡, so sÃ¡nh vÃ  tá»•ng há»£p thÃ´ng tin tá»« nhiá»u tÃ i liá»‡u náº¿u cáº§n thiáº¿t Ä‘á»ƒ táº¡o ra cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c vÃ  Ä‘áº§y Ä‘á»§ nháº¥t.

    *NguyÃªn táº¯c báº¯t buá»™c*:
    1. Tá»« táº¥t cáº£ cÃ¡c tÃ i liá»‡u tham kháº£o, báº¡n cáº§n Ä‘á»c háº¿t má»™t cÃ¡ch chi tiáº¿t cÃ¡c tÃ i liá»‡u Ä‘Ã³ sau Ä‘Ã³ xÃ¡c Ä‘á»‹nh cÃ¢u há»i cÃ³ liÃªn quan Ä‘áº¿n táº¥t cáº£ tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p khÃ´ng vÃ  tráº£ lá»i cÃ¢u há»i má»™t cÃ¡ch chÃ­nh xÃ¡c, Ä‘áº§y Ä‘á»§ nháº¥t.
    2. Náº¿u cÃ¢u há»i liÃªn quan Ä‘áº¿n cÃ¡c tÃ i liá»‡u hiá»‡n táº¡i, báº¡n cáº§n tráº£ lá»i dá»±a trÃªn cÃ¡c tÃ i liá»‡u Ä‘Ã£ Ä‘Æ°á»£c cung cáº¥p.
    3. Náº¿u cÃ¢u há»i khÃ´ng liÃªn quan Ä‘áº¿n tÃ i liá»‡u hiá»‡n táº¡i, báº¡n váº«n cÃ³ thá»ƒ tráº£ lá»i báº±ng kiáº¿n thá»©c chung, nhÆ°ng pháº£i má»Ÿ Ä‘áº§u rÃµ rÃ ng:
    â†’ "CÃ¢u há»i nÃ y khÃ´ng náº±m trong cÃ¡c tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p. TÃ´i sáº½ sá»­ dá»¥ng kiáº¿n thá»©c chung Ä‘á»ƒ tráº£ lá»i:"
    4. Sá»­ dá»¥ng font Unicode tiÃªu chuáº©n.
    5. Náº¿u ngÆ°á»i dÃ¹ng há»i báº±ng ngÃ´n ngá»¯ khÃ¡c, khÃ´ng pháº£i tiáº¿ng Viá»‡t, hÃ£y há»i láº¡i lá»‹ch sá»±:
    â†’ "Báº¡n cÃ³ muá»‘n tÃ´i tráº£ lá»i báº±ng tiáº¿ng Viá»‡t khÃ´ng?"
    6. Náº¿u ngÆ°á»i dÃ¹ng há»i cÃ¡c cÃ¢u nhÆ° "tiáº¿p Ä‘i", "tiáº¿p tá»¥c Ä‘i", "báº¡n cÃ³ thá»ƒ tráº£ lá»i láº¡i cÃ¢u há»i vá»«a rá»“i khÃ´ng", "váº­y cÃ²n...", "cÃ²n ná»¯a khÃ´ng?", "tiáº¿p tá»¥c nhÃ©", "tiáº¿p tá»¥c nÃ o", "tiáº¿p tá»¥c thÃ´i", "tiáº¿p tá»¥c nÃ o báº¡n Æ¡i", "tiáº¿p tá»¥c Ä‘i báº¡n Æ¡i", "tiáº¿p tá»¥c Ä‘i nÃ o báº¡n Æ¡i", "tiáº¿p tá»¥c Ä‘i nÃ o", "tiáº¿p tá»¥c Ä‘i nhÃ©", "tiáº¿p tá»¥c Ä‘i nha", "tiáº¿p tá»¥c Ä‘i báº¡n nha", "tiáº¿p tá»¥c Ä‘i báº¡n Æ¡i nha", "tiáº¿p tá»¥c Ä‘i báº¡n Æ¡i nhÃ©"... thÃ¬ báº¡n pháº£i dá»±a vÃ o lá»‹ch sá»­ há»™i thoáº¡i phÃ­a trÃªn Ä‘á»ƒ tráº£ lá»i Ä‘Ãºng ngá»¯ cáº£nh, khÃ´ng Ä‘Æ°á»£c tráº£ lá»i chung chung hoáº·c láº·p láº¡i ná»™i dung khÃ´ng liÃªn quan.

    ğŸ“… NgÃ y hiá»‡n táº¡i: {current_date}
    ğŸ« TrÆ°á»ng: Äáº¡i há»c CÃ´ng nghá»‡ ThÃ´ng tin - ÄHQG TP.HCM (UIT)
    ğŸ“š Danh sÃ¡ch cÃ¡c tÃ i liá»‡u tham kháº£o (cÃ¡c Ä‘oáº¡n liÃªn quan nháº¥t):
    {docs_summary}

    ğŸ“‚ Ná»™i dung Ä‘áº§y Ä‘á»§ cá»§a cÃ¡c file tÃ i liá»‡u nguá»“n (hÃ£y Ä‘á»c ká»¹ Ä‘á»ƒ tráº£ lá»i Ä‘áº§y Ä‘á»§ nháº¥t):
    {full_markdown_content}
    '''

    history_text = ""
    if history:
        for msg in history:
            if hasattr(msg, 'user_message') and hasattr(msg, 'bot_response'):
                history_text += f"NgÆ°á»i dÃ¹ng: {msg.user_message}\n"
                history_text += f"Bá»‘p Assistant: {msg.bot_response}\n"
            elif isinstance(msg, dict):
                if msg.get('role') == 'user':
                    history_text += f"NgÆ°á»i dÃ¹ng: {msg.get('content', '')}\n"
                else:
                    history_text += f"Bá»‘p Assistant: {msg.get('content', '')}\n"

        full_content = base_prompt + "\n" + history_text + f"\nNgÆ°á»i dÃ¹ng: {user_message}\nBá»‘p Assistant:"

    print('---DEBUG: history_text---')
    print(repr(history_text))
    if not history_text:
        print("KhÃ´ng cÃ³ lá»‹ch sá»­ há»™i thoáº¡i trÆ°á»›c Ä‘Ã³.")
        full_content = base_prompt + f"\nNgÆ°á»i dÃ¹ng: {user_message}\nBá»‘p Assistant:"
        
    print(f"â±ï¸ Thá»i gian truy váº¥n (Qdrant): {t1 - t0:.3f} giÃ¢y")
    print(f"â±ï¸ Thá»i gian augmentation (load markdown & chuáº©n bá»‹ prompt): {t2 - t1:.3f} giÃ¢y")

    response = model.generate_content(full_content, generation_config=generation_config)
    final_text = response.text
    return final_text

# --- API Views ---
@api_view(['GET', 'POST'])
def conversation_handler(request):
    user_id = request.headers.get('X-User-ID')
    if request.method == "GET":
        conversation_index = request.GET.get("conversation_index")
        conversation = get_object_or_404(Conversation, conversation_index=conversation_index, user_id=user_id)
        messages = ChatMessage.objects.filter(conversation=conversation).order_by("index")
        return Response({
            "conversation_index": conversation.conversation_index,
            "messages": ChatMessageSerializer(messages, many=True).data,
            "user_id": user_id
        })

    elif request.method == "POST":
        message = request.data.get('message', '')
        # user_id Ä‘Ã£ Ä‘Æ°á»£c láº¥y á»Ÿ trÃªn, chá»‰ generate khi khÃ´ng truyá»n lÃªn (reload trang)
        last_conv = Conversation.objects.filter(user_id=user_id).order_by('-created_at').first()
        if last_conv:
            conversation = last_conv
        else:
            conversation = Conversation.objects.create(user_id=user_id)

        current_message_count = ChatMessage.objects.filter(conversation=conversation).count()
        history_messages = ChatMessage.objects.filter(conversation=conversation).order_by("index")
        new_index = current_message_count
        response = get_chat_response(message, history_messages)
        chat = ChatMessage.objects.create(
            conversation=conversation, index=new_index,
            user_message=message, bot_response=response
        )
        return Response({
            "conversation_index": conversation.conversation_index,
            "chat": ChatMessageSerializer(chat).data,
            "user_id": user_id
        })